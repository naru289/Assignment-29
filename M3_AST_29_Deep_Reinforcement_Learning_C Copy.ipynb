{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-29/blob/main/M3_AST_29_Deep_Reinforcement_Learning_C%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaSXA3KF57Ko"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 29: Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8abyi0q57Kr"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq4Ny4Jm57Ks"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* setup the OpenAI Gym environment\n",
        "* create simple and neural network policies for cart-pole environment\n",
        "* implement policy gradients to solve cart-pole environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emcLfpQ-hDl6"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "FOyrjcBne1oN",
        "outputId": "ea30d88f-4860-4e80-9978-b33200a283a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=2360\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_29_Deep_Reinforcement_Learning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx pip3 install PyVirtualDisplay\")\n",
        "    ipython.magic(\"sx sudo apt-get install xvfb\")\n",
        "    ipython.magic(\"sx sudo apt-get install python-opengl\")\n",
        "    ipython.magic(\"sx sudo apt-get install ffmpeg\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsjutCK457Kt"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPM_q921EO00"
      },
      "source": [
        "### Reinforcement Learning (RL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDfRUG8EfZQ"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://docs.aws.amazon.com/deepracer/latest/developerguide/images/deepracer-reinforcement-learning-overview.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "In Reinforcement Learning, a software **agent** makes observations (**states**) and takes **actions** within an **environment**, and in return it receives **rewards**. Its objective is to learn to act in a way that will maximize its expected rewards over time. We can think of positive rewards as pleasure and negative rewards as pain. In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeUxjOROMkUP"
      },
      "source": [
        "### Introduction to OpenAI Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJk-xVkONpWK"
      },
      "source": [
        "One of the challenges of Reinforcement Learning is that in order to train an agent, we first need to have a working environment. If we want to program an agent that will learn to play an Atari game, we will need an Atari game simulator.\n",
        "\n",
        "**OpenAI Gym** is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on), so we can train agents, compare them, or develop new RL algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-jP78yVQ9Qh"
      },
      "source": [
        "**Create a CartPole environment**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Cart_pole.JPG\" width=400px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "It is a 2D simulation in which a pole is attached to a cart placed on a frictionless track. The agent has to apply force to move the cart left or right as shown in the figure above. **It is rewarded for every time step the pole remains upright**. The agent, therefore, must learn to keep the pole from falling over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-JfZNo8Stta"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhmlcSPZYaek"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib as mpl\n",
        "mpl.rc('animation', html='jshtml')\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPJrfe7bSNPv"
      },
      "source": [
        "We can get the list of all available environments by running the below code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okdia0OwSrYg"
      },
      "outputs": [],
      "source": [
        "# List all the available environments\n",
        "gym.envs.registry.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnXT_bVPTWrD"
      },
      "source": [
        "An environment can be created using `make()` function. After the environment is created, we must initialize it using the `reset()` method. This returns the first observation. Observations depend on the type of environment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cart-Pole is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright."
      ],
      "metadata": {
        "id": "r0K9bQMLv4Hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01b2q65yTzCB"
      },
      "outputs": [],
      "source": [
        "# Create cartpole environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Let's initialize the environment by calling reset() method. This returns an observation:\n",
        "obs = env.reset()\n",
        "obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSF9J4emSrs8"
      },
      "source": [
        "For the CartPole environment, each observation is a 1D NumPy array containing four floats as shown in the figure below.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Cartpole_image.JPG\" width=400px/>\n",
        "</center>\n",
        "\n",
        "These floats represent:\n",
        "\n",
        "* the cart’s horizontal position (0.0 = center),\n",
        "* its velocity (positive means right),\n",
        "* the angle of the pole (0.0 = vertical), and\n",
        "* its angular velocity (positive means clockwise).\n",
        "\n",
        "Now let’s display this environment and can be visualized by calling its `render()` method. If we want `render()` to return the rendered image as a NumPy array, we can set `mode=\"rgb_array\"` and visualize that array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fN1tWnwWYeR"
      },
      "outputs": [],
      "source": [
        "display = Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "# In this example we will set mode=\"rgb_array\" to get an image of the environment as a NumPy array\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "print(img.shape) # height, width, channels (3 = Red, Green, Blue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tDFAbmbRKzj"
      },
      "outputs": [],
      "source": [
        "# Create a function to plot environment\n",
        "def plot_environment(env, figsize=(5,4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how to interact with an environment. Your agent will need to select an action from an \"action space\" (the set of possible actions). Let's see what this environment's action space looks like:"
      ],
      "metadata": {
        "id": "zggFsmTtw7Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions possible\n",
        "env.action_space"
      ],
      "metadata": {
        "id": "a4kuZW3mw_vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, just two possible actions"
      ],
      "metadata": {
        "id": "krgw72jjxFMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discrete(2) means that the possible actions are integers 0 and 1, which represent accelerating left (0) or right (1). Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous). Since the pole is leaning toward the right (obs[2] > 0), let’s accelerate the cart toward the right."
      ],
      "metadata": {
        "id": "JqHS4DOcsBl_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFyvf2bUcmHI"
      },
      "source": [
        "Now we are ready to play the game. We use a simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Play the game with random agent\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "print(\"Action space: \", env.action_space)\n",
        "frames = []\n",
        "\n",
        "while True:\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    # Agent goes here\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "xJlB3kP9SLpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PYzYYrzol1q"
      },
      "source": [
        "In the above code cell,\n",
        "\n",
        "* **env.render()**: is used to display the environment image\n",
        "\n",
        "* **env.action_space**: gives the possible actions, here Discrete(2) means integers 0 and 1, which represent accelerating left (0) or right (1)\n",
        "\n",
        "* **.sample()**: is used to choose an action either 0 or 1 randomly\n",
        "\n",
        "* **env.step(action)**: executes the given action and returns four values:\n",
        "    \n",
        "    * **observation**: This is the new observation.\n",
        "\n",
        "    * **reward**: In this environment, we get a reward of 1.0 at every step, no matter what we do, so the goal is to keep the episode running as long as possible.\n",
        "    \n",
        "    * **done**: This value will be True when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, we have won). After that, the environment must be reset before it can be used again.\n",
        "\n",
        "    * **info**: This environment-specific dictionary can provide some extra information that we may find useful for debugging or for training. For example, in some games it may indicate how many lives the agent has.\n",
        "\n",
        "Let's visualize the animation of this simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69TwJCPqT6j7"
      },
      "outputs": [],
      "source": [
        "# Create functions to plot animation\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(fig, update_scene, fargs=(frames, patch),\n",
        "                                   frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpwGFKt1T9Ls"
      },
      "outputs": [],
      "source": [
        "# Visualize animation\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRQv8X15jWmY"
      },
      "source": [
        "From the above animation, we can see that the random agent keeps the pole up for few timesteps then it falls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnq1pgRxrjAo"
      },
      "source": [
        "### Policy Search\n",
        "\n",
        "The algorithm, a software agent uses to determine its actions is called its **policy**. A policy is an agent's strategy.\n",
        "\n",
        "For example, imagine a world where a robot moves across the room and the task is to get to the target point `(x, y)`, where it gets a reward. Here:\n",
        "\n",
        "*   A room is an `environment`\n",
        "*   Robot's current position is a `state`\n",
        "*   A `policy` is what an agent does to accomplish this task:\n",
        "\n",
        "    * dumb robots just wander around randomly until they accidentally end\n",
        "up in the right place (policy #1)\n",
        "\n",
        "    * others may, for some reason, learn to go along the walls most of the route (policy #2)\n",
        "\n",
        "    * smart robots plan the route in their \"head\" and go straight to the goal (policy #3)\n",
        "\n",
        "Obviously, some policies are better than others, and there are multiple ways to assess them, namely state-value function and action-value function. The goal of RL is to learn the best policy. A policy defines the learning agent's way of behaving at a given time.\n",
        "\n",
        "\n",
        "The policy could be a neural network taking observations as inputs and giving an output in terms of the action to take.\n",
        "\n",
        "The policy can be any algorithm you can think of, and it does not have to be `deter‐ministic`. In fact, in some cases it does not even have to observe the environment! For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability `p` every second, or randomly rotate left or right with probability `1 – p`. The rotation angle would be a random angle between –r and +r. Since this policy involves some randomness, it is called a `stochastic policy`. The robot will have an erratic trajectory, which guarantees that it will eventually get to any place it can reach and pick up all the dust. The question is, how much dust will it pick up in 30 minutes?\n",
        "\n",
        "\n",
        "How would you train such a robot? There are just two policy parameters\n",
        "you can tweak: the probability `p` and the angle range `r`. One possible learning algorithm could be to try out many different values for these parameters, and pick the combination that performs best. This is an example of **policy search**, in this case using a brute force approach. When the\n",
        "policy space is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYeu_4Ujb4C"
      },
      "source": [
        "Now we will solve the cart pole environment using different methods including:\n",
        "\n",
        "* **Simple Hard-coded policy:** Take the action of accelerating the cart left when the pole is leaning toward the left and accelerating it right when the pole is leaning toward the right.\n",
        "\n",
        "* **Neural Network Policy:** Use a neural network which takes an observation as input and outputs the probability of action to be executed. Then we select an action randomly according to the estimated probabilities.\n",
        "\n",
        "* **Policy gradients:** In Policy gradient methods, the process of action selection at every step is stochastic. It is based on the probability of\n",
        "selection of a particular action in each state. This can be useful in many applications where determining the accurate value function is complex. In case of cart-pole balancing problem, one such example is the upright state, where the pole is in the upright position but the agent must take either action defined by the objective. In this context, the agent may not prefer a deterministic action as it may limit the exploration across the state space.\n",
        "\n",
        "\n",
        "To know more about RL algorithms applied to the Cart-Pole environment, click [here](http://azadproject.ir/wp-content/uploads/2014/07/2017-Comparison-of-Reinforcement-Learning-Algorithms-applied-to-the-Cart-Pole-Problem.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw67iU36_pCz"
      },
      "source": [
        "**Simple Hard-coded Policy**\n",
        "\n",
        "Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward the right. We will run this policy to see the average rewards (time steps for which the pole remains upright) it gets over 500 episodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku90qWiVhW0g"
      },
      "outputs": [],
      "source": [
        "# Create environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Create a simple policy function\n",
        "def basic_policy(obs):\n",
        "    ''' Returns 0 (move cart left) if angle < 0 (pole leaning left) else 1 '''\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "# Run the simple policy for 500 episodes\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDfmQNtR_0-S"
      },
      "outputs": [],
      "source": [
        "# Statistics of the total rewards from 500 episodes\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "From the above results, we can see that for simple hardcoded policy even with 500 tries, this policy never managed to keep the pole upright for more than 71 consecutive steps. This environment is considered solved when the agent keeps the poll up for 200 steps.\n",
        "\n",
        "Let's visualize one episode:"
      ],
      "metadata": {
        "id": "ATiPP4b0wtMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT6_1tKSib_o"
      },
      "outputs": [],
      "source": [
        "# Visualize one episode with basic policy\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "frames = []\n",
        "\n",
        "for step in range(200):\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    # action based on simple policy\n",
        "    action = basic_policy(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Visualize animation\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "326vGSEqibbL"
      },
      "source": [
        "If we look at the simulation above, we will see that the cart oscillates left and right more and more strongly until the pole tilts too much.\n",
        "\n",
        "Let’s see if we can use neural networks to come up with a better policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3oFewc--5op"
      },
      "source": [
        "**Neural Network Policies**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Neural_network_policy.JPG\" width=600px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "As shown in the above figure, the neural network will take an observation as input, and it will output the action to be executed. More precisely, it will estimate a probability for each action, and then we will select an action randomly, according to the estimated probabilities.\n",
        "\n",
        "In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron. It will output the probability $p$ of action $0$ (left), and the probability of action $1$ (right) will be $1 – p$.\n",
        "\n",
        "We are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score because this approach lets the agent find the right balance between ***exploring*** new actions and ***exploiting*** the actions that are known to work well.\n",
        "\n",
        "Let's build the neural network policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGcsn_M7HXf9"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "n_inputs = 4 # == env.observation_space.shape[0]\n",
        "model = Sequential([\n",
        "                    Dense(5, activation=\"relu\", input_shape=[n_inputs]),\n",
        "                    Dense(1, activation=\"sigmoid\")\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXREglsI8-G"
      },
      "source": [
        "The number of inputs is the size of the observation space, and we have five hidden units. Finally, we want to output a single probability, so we have a single output neuron using the sigmoid activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCpaNRLXl7mu"
      },
      "source": [
        "Let's write a small function that will run the model to play one episode, and return the frames so we can display an animation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeCS3jLwI0X1"
      },
      "outputs": [],
      "source": [
        "# Create function to return frames for animation\n",
        "def render_policy_net(model, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        img = env.render(mode=\"rgb_array\")\n",
        "        frames.append(img)\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeqzRzxpm3QM"
      },
      "source": [
        "Now let's look at how well this randomly initialized policy network performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1qYB15ZlPIz"
      },
      "outputs": [],
      "source": [
        "# Visualize animation\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqb-N168m7aa"
      },
      "source": [
        "We can see it performs pretty bad. The neural network will have to learn to do better. First, let's see if it is capable of learning the basic policy we used earlier: go left if the pole is tilting left, and go right if it is tilting right.\n",
        "\n",
        "We can make the same network play in 50 different environments in parallel (this will give us a diverse training batch at each step), and train for 5000 iterations. We also reset environments when they are done. We train the model using a custom training loop so we can easily use the predictions at each training step to advance the environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnG07GbXlQdk"
      },
      "outputs": [],
      "source": [
        "# Play in 50 different environments\n",
        "n_environments = 50\n",
        "n_iterations = 5000\n",
        "\n",
        "envs = [gym.make(\"CartPole-v1\") for _ in range(n_environments)]\n",
        "for index, env in enumerate(envs):\n",
        "    env.seed(index)\n",
        "np.random.seed(42)\n",
        "observations = [env.reset() for env in envs]\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # if angle < 0, we want proba(left) = 1., or else proba(left) = 0.\n",
        "    target_probas = np.array([([1.] if obs[2] < 0 else [0.])\n",
        "                              for obs in observations])\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_probas = model(np.array(observations))\n",
        "        loss = tf.reduce_mean(loss_fn(target_probas, left_probas))\n",
        "    print(\"\\rIteration: {}, Loss: {:.3f}\".format(iteration, loss.numpy()), end=\"\")\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    actions = (np.random.rand(n_environments, 1) > left_probas.numpy()).astype(np.int32)\n",
        "    for env_index, env in enumerate(envs):\n",
        "        obs, reward, done, info = env.step(actions[env_index][0])\n",
        "        observations[env_index] = obs if not done else env.reset()\n",
        "\n",
        "for env in envs:\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CUwgba0nTmK"
      },
      "outputs": [],
      "source": [
        "# Visualize animation\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLDP2-HHnWzC"
      },
      "source": [
        "From above it seems like it learned the policy correctly. Now let's see if it can learn a better policy on its own. One that does not wobble as much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFlDb-XFnrfW"
      },
      "source": [
        "**Policy Gradients with Neural Network (Optional)**\n",
        "\n",
        "To train the neural network we will need to define the target probabilities. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. To know whether an action is good or bad, the problem is most actions have delayed effects, so when we win or lose points in an episode, it is not clear which actions contributed to this result. This is called the ***credit assignment problem***.\n",
        "\n",
        "The _Policy Gradients_ algorithm tackles this problem by first playing multiple episodes, then making the actions in good episodes slightly more likely, while actions in bad episodes are made slightly less likely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk9g52ldjb4M"
      },
      "source": [
        "In other words, PG algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular class of PG algorithms, called REINFORCE algorithms, was introduced in 1992 by Ronald Williams. Here is one common variant:\n",
        "\n",
        "1. First, let the neural network policy play the game several times, and at each step, compute the gradients that would make the chosen action even more likely-but don’t apply these gradients yet.\n",
        "\n",
        "2. Once we have run several episodes, compute each action’s advantage.\n",
        "\n",
        "3. If an action’s advantage is positive, it means that the action was probably good, and we want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action’s advantage is negative, it means the action was probably bad, and we want to apply the opposite gradients to make this action slightly less likely in the future. The solution is simply to multiply each gradient vector by the corresponding action’s advantage.\n",
        "\n",
        "4. Finally, compute the mean of all the resulting gradient vectors, and use it to perform a Gradient Descent step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LyhOnIRjb4M"
      },
      "source": [
        "Let’s implement this algorithm. We will train the neural network policy we built earlier so that it learns to balance the pole on the cart.\n",
        "\n",
        "First, we need a function that will play one step. We will pretend for now that whatever action it takes is the right one so that we can compute the loss and its gradients (these gradients will just be saved for a while, and we will modify them later depending on how good or bad the action turned out to be):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms4TUwsenvIZ"
      },
      "outputs": [],
      "source": [
        "# Function explanation is given below\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "    return obs, reward, done, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R8PBCe3jb4M"
      },
      "source": [
        "Let’s walk through this function:\n",
        "\n",
        "* Within the GradientTape block, we start by calling the model,\n",
        "giving it a single observation (we reshape the observation so it becomes a batch\n",
        "containing a single instance, as the model expects a batch). This outputs the\n",
        "probability of going left.\n",
        "\n",
        "* Next, we sample a random float between 0 and 1, and we check whether it is\n",
        "greater than left_proba. The action will be False with probability left_proba,\n",
        "or True with probability 1 - left_proba. Once we cast this Boolean to a number, the action will be 0 (left) or 1 (right) with the appropriate probabilities.\n",
        "\n",
        "* Next, we define the target probability of going left: it is 1 minus the action (cast\n",
        "to a float). If the action is 0 (left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability will be 0.\n",
        "\n",
        "* Then we compute the loss using the given loss function, and we use the tape to\n",
        "compute the gradient of the loss with regard to the model’s trainable variables.\n",
        "Again, these gradients will be tweaked later, before we apply them, depending on\n",
        "how good or bad the action turned out to be.\n",
        "\n",
        "* Finally, we play the selected action, and we return the new observation, the\n",
        "reward, whether the episode is ended or not, and the gradients that we\n",
        "just computed.\n",
        "\n",
        "Now let’s create another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients for each episode and each step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2MHQzLojb4M"
      },
      "outputs": [],
      "source": [
        "# Create play multiple episodes function\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-FtBZhjb4N"
      },
      "source": [
        "This code returns a list of reward lists (one reward list per episode, containing one reward per step), as well as a list of gradient lists (one gradient list per episode, each containing one tuple of gradients per step and each tuple containing one gradient tensor per trainable variable).\n",
        "\n",
        "The algorithm will use the `play_multiple_episodes()` function to play the game\n",
        "several times (e.g., 10 times), then it will go back and look at all the rewards, discount them, and normalize them. To do that, we need a couple more functions:\n",
        "\n",
        "* the first will compute the sum of future discounted rewards at each step, and\n",
        "\n",
        "* the second will normalize all these discounted rewards (returns) across many episodes\n",
        "\n",
        "The **discount factor** essentially determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. If $\\gamma = 0$, the agent will be completely myopic and only learn about actions that produce an immediate reward. If $\\gamma = 1$, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
        "\n",
        "To know more about the discount factor in RL, click [here](https://towardsdatascience.com/penalizing-the-discount-factor-in-reinforcement-learning-d672e3a38ffe)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naTc5izxjb4N"
      },
      "outputs": [],
      "source": [
        "# Create functions to sum and normalize rewards\n",
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_rate\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uR4GFKTjb4N"
      },
      "source": [
        "Let’s check that this works:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/actions_return_.JPG\" width=450px/>\n",
        "</center>\n",
        "\n",
        "Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50 as shown in the figure above. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrCH51u1jb4N"
      },
      "outputs": [],
      "source": [
        "# Compute discounted rewards\n",
        "discount_rewards(rewards = [10, 0, -50], discount_rate = 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7qQ5UHbjb4O"
      },
      "outputs": [],
      "source": [
        "# Normalize discounted all rewards\n",
        "discount_and_normalize_rewards(all_rewards = [[10, 0, -50], [10, 20]], discount_rate = 0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uetms1fkjb4O"
      },
      "source": [
        "The call to `discount_rewards()` returns exactly what we expect.\n",
        "We can verify that the function `discount_and_normalize_rewards()` does indeed\n",
        "return the normalized action advantages for each action in both episodes. Notice that the first episode was much worse than the second, so its normalized advantages are all negative; all actions from the first episode would be considered bad, and conversely all actions from the second episode would be considered good.\n",
        "\n",
        "Now let’s define the hyperparameters to run the algorithm. We will run 150 training iterations, playing 10 episodes per iteration, and each episode will last at most 200 steps. We will use a discount factor of 0.95:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_O2Irhyjb4O"
      },
      "outputs": [],
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_rate = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADQINYIjb4O"
      },
      "source": [
        "We also need an optimizer and the loss function. We will use the binary cross-entropy loss function because we are training a binary classifier (there are two possible actions: left or right):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PbbTyxUjb4P"
      },
      "outputs": [],
      "source": [
        "# Create optimizer and loss function\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSTq9sFQjb4P"
      },
      "source": [
        "Let's build and run the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vfISct9jb4P"
      },
      "outputs": [],
      "source": [
        "# Run training loop\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(42);\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # Play multiple episodes\n",
        "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    total_rewards = sum(map(sum, all_rewards))\n",
        "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
        "    # Normalize discounted rewards\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
        "    all_mean_grads = []\n",
        "    # Compute weighted mean of gradients for every trainable variable\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index]\n",
        "                                     for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                                     for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsY31uwqjb4Q"
      },
      "source": [
        "Let’s walk through this code:\n",
        "\n",
        "* At each training iteration, this loop calls the `play_multiple_episodes()` function, which plays the game 10 times and returns all the rewards and gradients for every episode and step.\n",
        "\n",
        "* Then we call the `discount_and_normalize_rewards()` to compute each action’s\n",
        "normalized advantage. This provides a measure of how good or bad each action actually was, in hindsight.\n",
        "\n",
        "* Next, we go through each trainable variable, and for each of them we compute\n",
        "the weighted mean of the gradients for that variable over all episodes and all\n",
        "steps, weighted by the `final_reward`.\n",
        "\n",
        "* Finally, we apply these mean gradients using the optimizer: the model’s trainable variables will be tweaked, and hopefully the policy will be a bit better.\n",
        "\n",
        "This code will train the neural network policy, and it will successfully learn to balance the pole on the cart. The mean reward per episode will get very close to 200 which represents success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdT58imXjb4Q"
      },
      "outputs": [],
      "source": [
        "# Visualize PG algorithm animation\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VsARaNlEmsrc"
      },
      "source": [
        "#@title Q1. Which of the following is correct about Reinforcement Learning ?\n",
        "Answer1 = \"The agent makes an action in an environment and is given back a new observation and a reward for that action\" #@param [\"\", \"The agent makes an action in an environment and is given back a new observation and a reward for that action\", \"The agent makes an observation in an environment and is given back a new action and a reward for that observation\", \"The agent is given a new environment whenever it receives a reward for its action\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following statements and answer Q2.\n",
        "\n",
        "A. A policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n",
        "\n",
        "B. The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.\n",
        "\n",
        "C. A model of the environment allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_F3RfGojqZDc"
      },
      "source": [
        "#@title Q.2. Which of the following statements is/are True for a Reinforcement Learning Agent ?\n",
        "Answer2 = \"A, B and C\" #@param [\"\",\"Only A\", \"Only C\", \"Both A and B\",\"Both B and C\",  \"A, B and C\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "outputId": "b297052e-e47c-4031-9fff-5915123947bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first..\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2360\n",
            "Date of submission:  19 Aug 2023\n",
            "Time of submission:  16:14:06\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}